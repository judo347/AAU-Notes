##### *Usability*

Describes a quality in the design: "*The effectiveness, efficiency and satisfaction with which specified users can achieve specified goals in particular environments.*"

**Effectiveness:** the accuracy and completeness with which specified users can achieve specified goals in particular environments.

**Efficiency:** the resource expended in relation to the accuracy and completeness of goals achieved.

**Satisfaction:** the comfort and acceptability of the work system to its users and other people affected it its use.

#### Jakob Nielsen describes Usability as:

**Learnability:** How easy is it for users to accomplish basic tasks the first time they encounter the design?

**Efficiency:** Once users have learned the design, how quickly can they perform tasks?

**Memorability:** When users return to the design after a period of not using it, how easily can they reestablish proficiency?

**Errors:** How many errors do users make; how severe are these errors, and how easily can they recover from the errors?

**Satisfaction:** How pleasant is it to use the design?

#### Usability Testing

The study of users' interaction with a product.

Purpose:

- Identifying usability problem in a system
- Starting point for refinements of design.

Outcome:

- A ranked list of usability problems
- Knowledge about what works well.

#### How do we evaluate usability:

Inquiry (We try to understand users)

- PACT and particular gathering method. We need to understand the users in order to design and test  a program.

Testing

- Users test product design. Could be in an inspection room. (Like in Silicon Valley)

Inspection

- Testing of a design by an expect. So an expect would go trough the program and try to spot usability programs.

#### When to test?

![](.\img\10.png)

#### lab vs field test

Lab strengths

- The least obtrusive way to collect data.
- Allows communication "behind the scene".
- Allows many observers.
- High replicability and control.
- Demand characteristics.

Lab weaknesses

- Somewhat "sterile" environment.
- Test participants may feel like "lab monkeys".
- Questionable realism (ecological validly).

#### Usability testing

Representative users interact with design.

Task solving and/or "thinking-aloud".

Produces a ranked list of usability problems.

Pros

- Identifies problems very precisely.
- Gives first-hand insight into use.

Cons

- Test situation can be unnatural.
- Difficult and very time consuming.

#### Heuristic Inspection

Usability experts inpect a design using a checklist (heuristic).

Scenarios + relevant tasks can strucuture process.

Produces a ranked list of usablity problems.

Pros

- Quick and easy to conduct.
- No users required.
- 3-5 inspections finds 70% of all problems.

Cons

- High proportion of "cosmetic" problems.
- "False" usability problems.

#### The usability testing process (participants perspective)

Introduction -> solve tasks -> debriefing.

#### The usability testing process (our perspective)

Make sure everything is ready -> observation + data collection -> analysis -> problem list.

#### Activities in Usability Testing

![](.\img\11.png)

#### Below is further explanation of the above image:

**Planning: Context of use**

- Where is the design/system used?
  - Physical environment?
  - Social context?
- Who uses the design?
  - User profiles
- Why is the design used?
  - What do people use it for?
  - Work? Leisure? Other activities?
- How is the design used?
  - Typical interactions.
  - Relevant and realistic data.

**Planning: Test participants**

- Representative for the user group
  - Demographics.
  - Experience.
- Number of test subjects
  - Generalizability.
  - Quantitative conclusions.
  - Statistics.
- Using fellow students
  - Problematic.
  - Motivation.
  - Demographics and experience.

**Planning: Number of test participants**

- Number of participants vs Usability problems found: 3 = 60%, 4 = 75%, 7 = 90%, 15 = 100%.

**Planning: Decide on the tasks**

What are the basic tasks that representative users do with the system?

Is the while system part of the evaluation?

Can we create crystal clear tasks descriptions?

How long does it take to solve the tasks?

Some useful rules for defining tasks:

1. Make the tasks realistic.
2. Make the tasks actionable.
3. Avoid clues and descripting the steps.

Good tasks

- Represent real use of the system.
- Decribe the end reuslt.
- Motivate (why should they be solved?).
- Include relevant data (e.g. names).
- Group smaller sub-tasks together.

Typical problems

- Vague, unclear or general.
- Provides too much help.
- Contain jargon and unfamiliar terms.
- Forces the user into a specific sequence.

**Planning: Decide on what to measure and how**

Are all the components of usability relevant?

How will we collect data?

What are we going to measure?

Think aloud?

**Example of usability metrics**

Objective metrics:

- Effectiveness: how many tasks were completed.
- Efficiency: how fast were they completed.

Subjective (perceived) usability metrics

- Interview data
- Questionnaires (for example SUS, USE questionnaires)

![](.\img\12.png)

**Video recordings**

The whole scene? The screen? The surroundings?

**Equipment in the lab**

Motorized cameras, pc screen capture, camera remote control, video monitors, video mixer, quad-spiller, microphones and video recorders (DVD).

Or you could use your room and some recording software.

![](.\img\13.png)

**Debriefing**

Is done immediately after evaluation session.

Can include:

- Filling out a questionnaire with options.
- An interview: explaining particular events in the evaluation.
- Critiquing the interaction design.
- User suggestion solutions and design ideas.

Allow enough time for discussion.

#### What is a usability problem?

For user-based evaluations:

A problem experienced by a specific user while interacting with a specific system.

The user...

- is deployed or prevented in completing a task
- feels frustrated
- makes mistakes
- misses important information
- stops talking
- is confused or surprised
- changes strategy or approach
- asks for help
- makes negative comments

For expert-based evaluations:

A *potential* problem identified by a specific expert to be in conflict with a specific heuristic or guideline.

Task 1: "You need to collaborate with another person to create an invitation. Log in to the application and share your credentials with a contact of yours."

Problem(s):

- Does not understand the difference between the concepts of "collaboration" vs. "invitation" is this context?
- Does not notice the [Collaborate] option?
- Misreading the task?

Task 2: "Create an invitation and invite your guests."

Problem(s):

- Unclear meaning of [Preview current invitation]
- Does not notice the [Send invitations] options
- Unclear what "QR code" means

#### Good and bad behaviour of the test moderator

Important characteristics:

- Solid knowledge about usability
- Fast learner
- Can establish good relations to subjects
- Good memory
- Good at listening
- Good at communicationg
- Can handle uncertainty
- Flexible and capable of improvasing
- Can stay alert for a long time
- Can maintain and overview

Typical problems:

- Controlling rather than supporting
- Too focused on data collection
- Stick too close to test plan
- Appears better knowing
- Does not establish good relations
- Jumps to conclusions

#### The log file

The log file is an important document in the analysis:

- Simplified transcriptions of the evaluation
- Captures the user interaction in textual form
- Provides overview and "filtered detail"
- Relation between time, events, and usability problems
- Specialized tools exists - but a table in Word will do
- Can be created "live" during tests, and fished from video recordings

#### The problem list

The problem list is the primary outcome from an evaluation:

- A ranked, and numbered, list of usability problems
- Indicates how many users experienced the problem (and who)
- Indicates where in the system the problem was experienced
- Describes each identified usability problem in detail and with brief examples
- The problem/description may go across several tasks
- May be divided into two lists: 1) overview and 2) detail

![](.\img\14.png)

#### categorizing problems

Typically three categories:

- Critical, Serious, Cosmetic
- In case of different experience of severity, you typically chose the most severe

Critical problem

- Usable to continue
- Feels the system behaves strongly irritating
- Critical difference between believed and actual state of the system

Catastrophic

- More than one user experienced the same critical problem independently

![](.\img\15.png)

#### Summary example

Overall performance

- Test subjects spent 36-53 minutes
- No one were able to solve all tasks
- High level of stress and frustration

Total of 83 usability problems:

- 25 critical
- 45 serious
- 13 cosmetic
- 31 "unique" problems